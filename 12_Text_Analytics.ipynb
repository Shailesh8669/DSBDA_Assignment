{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b35f74a-3759-46d4-9470-f957213511be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\omsaw\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\omsaw\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\omsaw\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\omsaw\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\omsaw\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\omsaw\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc48e0a4-6d60-4b15-aeff-6b2da6be9979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\omsaw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\omsaw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\omsaw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\omsaw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Natural', 'Language', 'Processing', 'is', 'amazing', '!']\n",
      "After Stopword Removal: ['Natural', 'Language', 'Processing', 'amazing']\n",
      "Stemmed Words: ['natur', 'languag', 'process', 'amaz']\n",
      "Lemmatized Words: ['natural', 'language', 'processing', 'amazing']\n",
      "TF: {'natural': 0.25, 'language': 0.25, 'processing': 0.25, 'amazing': 0.25}\n",
      "IDF: {'natural': 0.4054651081081644, 'language': 0.4054651081081644, 'processing': 0.4054651081081644, 'amazing': 1.0986122886681098}\n",
      "TF-IDF: {'natural': 0.1013662770270411, 'language': 0.1013662770270411, 'processing': 0.1013662770270411, 'amazing': 0.27465307216702745}\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import and download NLTK resources\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "# Download required resources (run once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Step 2: Sample document\n",
    "text = \"Natural Language Processing is amazing!\"\n",
    "\n",
    "# Step 3: Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Step 4: Stop Words Removal (and removing punctuation)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered = [w for w in tokens if w.lower() not in stop_words and w.isalpha()]\n",
    "print(\"After Stopword Removal:\", filtered)\n",
    "\n",
    "# Step 5: Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(w) for w in filtered]\n",
    "print(\"Stemmed Words:\", stemmed)\n",
    "\n",
    "# Step 6: Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(w.lower()) for w in filtered]\n",
    "print(\"Lemmatized Words:\", lemmatized)\n",
    "\n",
    "# Step 7: Term Frequency (TF)\n",
    "tf_counts = Counter(lemmatized)\n",
    "total_terms = sum(tf_counts.values())\n",
    "tf = {word: count / total_terms for word, count in tf_counts.items()}\n",
    "print(\"TF:\", tf)\n",
    "\n",
    "# Step 8: Inverse Document Frequency (IDF)\n",
    "# Example: a small corpus with 3 documents\n",
    "documents = [\n",
    "    ['natural', 'language', 'processing', 'amazing'],\n",
    "    ['machine', 'learning', 'is', 'fun'],\n",
    "    ['natural', 'language', 'processing', 'applications']\n",
    "]\n",
    "\n",
    "N = len(documents)\n",
    "idf = {}\n",
    "for word in tf:\n",
    "    doc_count = sum(1 for doc in documents if word in doc)\n",
    "    idf[word] = math.log(N / doc_count)\n",
    "print(\"IDF:\", idf)\n",
    "\n",
    "# Step 9: TF-IDF Calculation\n",
    "tf_idf = {word: tf[word] * idf[word] for word in tf}\n",
    "print(\"TF-IDF:\", tf_idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b156ea81-e9ae-44b2-b88e-ded4c4255ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
